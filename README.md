# **Distributed Log Processing System with Apache Flume, Kafka, and Elasticsearch**

## **Project Overview**
This project implements a **distributed log processing system** using **Apache Flume**, **Kafka**, and **Elasticsearch**. It collects logs from multiple microservices, processes them through Kafka's Pub-Sub model, and stores them in Elasticsearch for real-time querying and analysis.

### **Key Features**
- **Microservices Log Generation**: Logs are generated by different services and categorized into logs, heartbeat, and alerts.
- **Log Aggregation using Flume**: Flume captures logs and pushes them to Kafka topics.
- **Kafka as a Pub-Sub Broker**: Kafka brokers the logs for scalable consumption.
- **Consumers for Storage & Alerting**: Consumers store logs in **Elasticsearch** and trigger alerts for critical logs.
- **Distributed Execution**: Microservices and Flume run on **VM1**, while Kafka and consumers run on **VM2**.

---

## **Project Structure**
```
Flume/
├── flume-kafka.conf  # Flume configuration file
├── flume.log          # Flume logs

microservice/
├── orderservice.py    # Order processing microservice
├── payment.py         # Payment processing microservice
├── registration.py    # User registration microservice
├── logs/
│   ├── alert_logs.log      # Alert logs
│   ├── heartbeat_logs.log  # Heartbeat logs
│   ├── microservices_logs.log  # General service logs
│
├── consumer1.py       # Kafka consumer for Elasticsearch
├── Commands.md        # Command list to start services
```

---

## **Setup Instructions**
### **Step 1: Clone the Repository**
```bash
git clone https://github.com/your-repo-name.git
cd your-repo-name
```

### **Step 2: Start Kafka Broker on VM2**
```bash
bin/zookeeper-server-start.sh config/zookeeper.properties &
bin/kafka-server-start.sh config/server.properties &
```

### **Step 3: Create Kafka Topics**
```bash
bin/kafka-topics.sh --create --topic logs --bootstrap-server <VM2_IP>:9092
bin/kafka-topics.sh --create --topic heartbeat --bootstrap-server <VM2_IP>:9092
bin/kafka-topics.sh --create --topic alerts --bootstrap-server <VM2_IP>:9092
```

### **Step 4: Start Flume on VM1**
```bash
flume-ng agent --conf conf --conf-file Flume/flume-kafka.conf --name agent1 -Dflume.root.logger=INFO,console
```

### **Step 5: Run Microservices on VM1**
```bash
python3 microservice/orderservice.py &
python3 microservice/payment.py &
python3 microservice/registration.py &
```

### **Step 6: Start Consumers on VM2**
```bash
python3 microservice/consumer1.py
```

### **Step 7: Verify Kafka Messages**
To check messages in Kafka topics:
```bash
bin/kafka-console-consumer.sh --bootstrap-server <VM2_IP>:9092 --topic logs --from-beginning
bin/kafka-console-consumer.sh --bootstrap-server <VM2_IP>:9092 --topic heartbeat --from-beginning
```

### **Step 8: Check Logs in Elasticsearch**
Run the following command to query Elasticsearch for logs:
```bash
curl -X GET "http://<VM2_IP>:9200/logs_index/_search?pretty"
```

---

## **How it Works**
1. **Microservices generate logs** in different categories (logs, heartbeat, alerts).
2. **Flume aggregates logs** and forwards them to Kafka topics.
3. **Kafka acts as a broker**, enabling scalable log processing.
4. **Consumers listen to Kafka topics** and store logs in Elasticsearch or trigger alerts.
5. **Elasticsearch indexes logs**, allowing efficient querying and analysis.

---
